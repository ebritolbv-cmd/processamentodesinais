{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 1: Classificação de Áudio com CNN e MFCC\n",
    "\n",
    "Este notebook contém a implementação completa para o **Desafio 1**, focado na classificação de sons ambientais do dataset **ESC-10** utilizando uma **Rede Neural Convolucional (CNN)**.\n",
    "\n",
    "## 1. Configuração do Ambiente e Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "\n",
    "# Configurações de áudio\n",
    "Fs = 44100\n",
    "n_mfcc = 40\n",
    "max_len = 431\n",
    "classes_names = [\"chainsaw\", \"crackling_fire\", \"dog\", \"rain\", \"sea_waves\", \"clock_tick\", \"crying_baby\", \"helicopter\", \"rooster\", \"sneezing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Processamento dos Dados\n",
    "Extraímos os MFCCs mantendo a estrutura 2D para que a CNN possa processá-los como imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_2d(file_path, n_mfcc=40, max_len=431):\n",
    "    audio, sr = librosa.load(file_path, sr=Fs)\n",
    "    target_length = 5 * Fs\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])))\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc\n",
    "\n",
    "# Download do dataset\n",
    "path = kagglehub.dataset_download(\"sreyareddy15/esc10rearranged\")\n",
    "base_path = os.path.join(path, \"Data\")\n",
    "\n",
    "X, y = [], []\n",
    "for label in classes_names:\n",
    "    folder_path = os.path.join(base_path, label)\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "    for f in files:\n",
    "        mfcc = extract_mfcc_2d(os.path.join(folder_path, f))\n",
    "        if mfcc is not None:\n",
    "            X.append(mfcc)\n",
    "            y.append(label)\n",
    "\n",
    "X = np.array(X)[..., np.newaxis]\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arquitetura da Rede Neural Convolucional (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(n_mfcc, max_len, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(classes_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento e Visualização dos Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plotagem dos Gráficos\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Gráfico de Acurácia\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Treino', color='blue')\n",
    "plt.plot(history.history['val_accuracy'], label='Validação', color='orange')\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico de Perda (Loss)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Treino', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validação', color='orange')\n",
    "plt.title('Perda do Modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
